\documentclass{report}
\usepackage[latin1]{inputenc}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\SweaveOpts{keep.source=TRUE}
%\usepackage{cancel}
\usepackage[top=3.5cm,left=3cm,right=3cm,bottom=2cm,foot=1cm]{geometry}
\renewcommand{\thesection}{\arabic{section}}

\title{\textbf{Stats 202} \\ Homework 5}
\author{\textbf{Joseph Marrama}}
\date{December 9, 2011}

\begin{document}
\maketitle

\section*{Problem 1}

We can convert the similarity matrix to a dissimilarity matrix with the formula $D_{ik} = (S_{ii} - 2*S_{ik} + S_{kk})^{0.5}$ in order to use the 'hclust' function.  

<<>>=
sim = matrix(c(1.0, 0.1, 0.42, 0.54, 0.35, 0.1, 1.0, 0.63, 0.46, 0.98, 0.42, 0.63, 1.0, 0.41, 0.85, 0.54, 0.46, 0.41, 1.0, 0.73, 0.35, 0.98, 0.85, 0.73, 1.0), 5)
dis = sqrt(2*(1-sim))
dist = as.dist(dis)
#compute the hierarchical clustering using the max distance between clusters
#, i.e. 'complete' hierarchical clustering
comp_clust = hclust(dist)
#compute the hierarchical clustering using the min distance
#, i.e. 'single' hierarchical clustering
sing_clust = hclust(dist, method='single')
@

\begin{figure}
\begin{center}
<<fig=true, width=10, height=7>>=
plot(comp_clust)
@
\caption{(Problem 1) The hierarchical clustering tree for complete clustering}
\end{center}
\end{figure} 

\begin{figure}
\begin{center}
<<fig=true, width=10, height=7>>=
plot(sing_clust)
@
\caption{(Problem 1) The hierarchical clustering tree for single clustering}
\end{center}
\end{figure} 

\section*{Problem 2}

<<>>=
centers = matrix(c(0,0,1,0,0,1), 3, 2, byrow=T)

generate = function(n=50, extradim=0, sigma=1, mu=7) { 

	 data1 = matrix(rnorm(n*2), n, 2) * sigma 
	 data1[,1] = data1[,1] + centers[1,1] * mu
	 data1[,2] = data1[,2] + centers[1,2] * mu

	 data2 = matrix(rnorm(n*2), n, 2) * sigma 
	 data2[,1] = data2[,1] + centers[2,1] * mu
	 data2[,2] = data2[,2] + centers[2,2] * mu

	 data3 = matrix(rnorm(n*2), n, 2) * sigma 
	 data3[,1] = data3[,1] + centers[3,1] * mu
	 data3[,2] = data3[,2] + centers[3,2] * mu

	 data = rbind(data1,data2,data3)
	 if (extradim > 0) {
     noise = matrix(rnorm(3*n*extradim)*sigma, 3*n, extradim)
     data = cbind(data, noise)
	 }
	 return(data)
}
@

With extradim=0, this function generates a $3*n$ by $2$ matrix, where the first $n$ rows are normally distributed with mean $c[,1]$, the second $n$ rows are normally distributed around $c[,2]$, and the last 50 rows are normally distributed around $c[,3]$. The parameter sigma controls the standard deviation of all 3 parts of the matrix, and the parameter mu scales the means of all 3 parts, and the parameter n scales the size of the matrix.

\subsubsection*{b}

The extradim argument adds on extradim dimensions to each data point made up of gaussian noise with mean 0 and standard deviation sigma. With extradim above 0, it might be harder to produce a good clustering, since the noise will equalize the data, i.e. decrease the distances between points drawn from different gaussians.

\subsubsection*{c}

<<>>=
data = generate()
a = 1:150
a = 1 + floor((a-1)/50)
plot(data, pch=23, bg=c('red','blue','green')[a])
@

\subsubsection*{d}

Mclust should make accurate predictions, return a model with a reasonable BIC score, and hopefully return a model with equal variance, as the original data has only one variance. 

<<>>=
library(mclust)
model = Mclust(data)
model$bic
model$modelName
@

\begin{figure}
\begin{center}
<<fig=true, width=10, height=7>>=
plot(data, pch=23, bg=c('red','blue','green')[model$classification])
@
\caption{(Problem 2.d) The classifications for the cluster model's predictions with no noise}
\end{center}
\end{figure} 

\subsubsection*{e}

It starts breaking down when extradim is around 40. The classification doesn't work, and the bic score gets very low.

<<>>=
nd = generate(extradim=40)
data = nd[,1:2]
model=Mclust(nd)
model$bic
@

\begin{figure}
\begin{center}
<<fig=true, width=10, height=7>>=
plot(data, pch=23, bg=c('red','blue','green')[model$classification])
@
\caption{(Problem 2.e) The classifications for the cluster model's predictions with extradim=40}
\end{center}
\end{figure} 

\subsubsection*{f}

Raising the value of mu seems to help, because it increases the distance between the means of each cluster. Also, lowering sigma also helps, because it decreases the variance between the inter-cluster distances and decreases the corruption of the noise.

<<>>=
nd = generate(extradim=50, mu = 10, sigma = 0.5)
data = nd[,1:2]
model=Mclust(nd)
model$bic
@

\begin{figure}
\begin{center}
<<fig=true, width=10, height=7>>=
plot(data, pch=23, bg=c('red','blue','green')[model$classification])
@
\caption{(Problem 2.f) The classifications for the cluster model's predictions with extradim=50, mu=10, and sigma=0.5}
\end{center}
\end{figure} 

\section*{Problem 3}

<<>>=
bounding_box = function(data) {

  rect = mat.or.vec(2, ncol(data))
  #loop through each dimension, find min and max
  for(i in 1:ncol(data)){
    rect[1, i] = min(data[,i])
    rect[2, i] = max(data[,i])
  }

  samples = mat.or.vec(nrow(data), ncol(data)) 
  for(i in 1:ncol(data)){
    samples[,i] = matrix(runif(nrow(samples), rect[1,i], rect[2,i]), nrow(samples))
  }

  return(samples)
}
@

\subsubsection*{b}

Matrix d will contain all datasets in different rows. Rows 1 to 150 will be the first, 151 to 300 will be the second, etc.

<<>>=
library(cluster)
#d = mat.or.vec(0,2)
m=list()
m$d1=bounding_box(generate())
m$d2=bounding_box(generate())
m$d3=bounding_box(generate())
m$d4=bounding_box(generate())
m$d5=bounding_box(generate())
m$d6=bounding_box(generate())
m$d7=bounding_box(generate())
m$d8=bounding_box(generate())
m$d9=bounding_box(generate())
m$m1=kmeans(m$d1, 1)
m$m2=kmeans(m$d2, 2)
m$m3=kmeans(m$d3, 3)
m$m4=kmeans(m$d4, 4)
m$m5=kmeans(m$d5, 5)
m$m6=kmeans(m$d6, 6)
m$m7=kmeans(m$d7, 7)
m$m8=kmeans(m$d8, 8)
m$m9=kmeans(m$d9, 9)
@

\subsubsection*{c}

(see the 9 figures below)

\begin{figure}
\begin{center}
<<fig=true, width=10, height=7>>=
plot(m$d1, pch=23, bg=c('red','blue','green', 'purple', 'yellow', 'black', 'grey', 'brown', 'pink')[m$m1$cluster])
@
\caption{(Problem 3.c) Kmeans predictions for 1 cluster on bounded box uniform data}
\end{center}
\end{figure} 

\begin{figure}
\begin{center}
<<fig=true, width=10, height=7>>=
plot(m$d2, pch=23, bg=c('red','blue','green', 'purple', 'yellow', 'black', 'grey', 'brown', 'pink')[m$m2$cluster])
@
\caption{(Problem 3.c) Kmeans predictions for 2 clusters on bounded box uniform data}
\end{center}
\end{figure} 

\begin{figure}
\begin{center}
<<fig=true, width=10, height=7>>=
plot(m$d3, pch=23, bg=c('red','blue','green', 'purple', 'yellow', 'black', 'grey', 'brown', 'pink')[m$m3$cluster])
@
\caption{(Problem 3.c) Kmeans predictions for 3 clusters on bounded box uniform data}
\end{center}
\end{figure} 

\begin{figure}
\begin{center}
<<fig=true, width=10, height=7>>=
plot(m$d4, pch=23, bg=c('red','blue','green', 'purple', 'yellow', 'black', 'grey', 'brown', 'pink')[m$m4$cluster])
@
\caption{(Problem 3.c) Kmeans predictions for 4 clusters on bounded box uniform data}
\end{center}
\end{figure} 

\begin{figure}
\begin{center}
<<fig=true, width=10, height=7>>=
plot(m$d5, pch=23, bg=c('red','blue','green', 'purple', 'yellow', 'black', 'grey', 'brown', 'pink')[m$m5$cluster])
@
\caption{(Problem 3.c) Kmeans predictions for 5 clusters on bounded box uniform data}
\end{center}
\end{figure} 

\begin{figure}
\begin{center}
<<fig=true, width=10, height=7>>=
plot(m$d6, pch=23, bg=c('red','blue','green', 'purple', 'yellow', 'black', 'grey', 'brown', 'pink')[m$m6$cluster])
@
\caption{(Problem 3.c) Kmeans predictions for 6 clusters on bounded box uniform data}
\end{center}
\end{figure} 

\begin{figure}
\begin{center}
<<fig=true, width=10, height=7>>=
plot(m$d7, pch=23, bg=c('red','blue','green', 'purple', 'yellow', 'black', 'grey', 'brown', 'pink')[m$m7$cluster])
@
\caption{(Problem 3.c) Kmeans predictions for 7 clusters on bounded box uniform data}
\end{center}
\end{figure} 

\begin{figure}
\begin{center}
<<fig=true, width=10, height=7>>=
plot(m$d8, pch=23, bg=c('red','blue','green', 'purple', 'yellow', 'black', 'grey', 'brown', 'pink')[m$m8$cluster])
@
\caption{(Problem 3.c) Kmeans predictions for 8 clusters on bounded box uniform data}
\end{center}
\end{figure} 

\begin{figure}
\begin{center}
<<fig=true, width=10, height=7>>=
plot(m$d9, pch=23, bg=c('red','blue','green', 'purple', 'yellow', 'black', 'grey', 'brown', 'pink')[m$m9$cluster])
@
\caption{(Problem 3.c) Kmeans predictions for 9 clusters on bounded box uniform data}
\end{center}
\end{figure} 

\subsubsection*{d}
\newpage

<<>>=
graph_gap = function(extradim = 0, K = 9){

#calc total log sum of squares for normal and uniform points
cluster_totss = mat.or.vec(1,K)
uniform_totss = mat.or.vec(1,K)
data = generate(extradim = extradim)
uniform = bounding_box(data)

for(k in 1:K){
  normal_kmean = kmeans(data, k)
  uniform_kmean = kmeans(uniform, k)
  #cluster_totss[k] = normal_kmean$totss
  #uniform_totss[k] = uniform_kmean$totss
  cluster_totss[k] = log(normal_kmean$tot.withinss)
  uniform_totss[k] = log(uniform_kmean$tot.withinss)
}

l = list()
l$cluster_totss = cluster_totss
l$uniform_totss = uniform_totss
l$gap = uniform_totss - cluster_totss
return(l)
}

gaps = graph_gap()
@

\begin{figure}
\begin{center}
<<fig=true, width=10, height=7>>=
plot(1:9, gaps$cluster_totss, col='blue', ylim=c(0,8), xlab='Number of clusters')
points(1:9, gaps$uniform_totss, col='red')
points(1:9, gaps$gap, col='black')
@
\caption{(Problem 3.d) The gap statistic in blue, and the in-cluster sum of squares for the normal data (blue) and uniform data (red)}
\end{center}
\end{figure} 

As you can tell, the largest gap (and by the gap statistic, optimal number of clusters) is at K = 3, which is in fact the "true" number of clusters

\subsubsection*{e}

<<>>=
gap10 = graph_gap(extradim = 10)
gap20 = graph_gap(extradim = 20)
gap30 = graph_gap(extradim = 30)
gap40 = graph_gap(extradim = 40)
gap50 = graph_gap(extradim = 50)
@


\begin{figure}
\begin{center}
<<fig=true, width=10, height=7>>=
plot(1:9, gap10$cluster_totss, col='blue', ylim=c(0,14), xlab='Number of clusters')
points(1:9, gap10$uniform_totss, col='red')
points(1:9, gap10$gap, col='black')
@
\caption{(Problem 3.e) The gap statistic in blue, and the in-cluster sum of squares for data with 10 extra dimensions of noise (blue) and uniform data (red)}
\end{center}
\end{figure} 


\begin{figure}
\begin{center}
<<fig=true, width=10, height=7>>=
plot(1:9, gap20$cluster_totss, col='blue', ylim=c(0,14), xlab='Number of clusters')
points(1:9, gap20$uniform_totss, col='red')
points(1:9, gap20$gap, col='black')
@
\caption{(Problem 3.e) The gap statistic in blue, and the in-cluster sum of squares for data with 20 extra dimensions of noise (blue) and uniform data (red)}
\end{center}
\end{figure} 

\begin{figure}
\begin{center}
<<fig=true, width=10, height=7>>=
plot(1:9, gap30$cluster_totss, col='blue', ylim=c(0,14), xlab='Number of clusters')
points(1:9, gap30$uniform_totss, col='red')
points(1:9, gap30$gap, col='black')
@
\caption{(Problem 3.e) The gap statistic in blue, and the in-cluster sum of squares for data with 30 extra dimensions of noise (blue) and uniform data (red)}
\end{center}
\end{figure} 

\begin{figure}
\begin{center}
<<fig=true, width=10, height=7>>=
plot(1:9, gap40$cluster_totss, col='blue', ylim=c(0,14), xlab='Number of clusters')
points(1:9, gap40$uniform_totss, col='red')
points(1:9, gap40$gap, col='black')
@
\caption{(Problem 3.e) The gap statistic in blue, and the in-cluster sum of squares for data with 40 extra dimensions of noise (blue) and uniform data (red)}
\end{center}
\end{figure} 

\begin{figure}
\begin{center}
<<fig=true, width=10, height=7>>=
plot(1:9, gap50$cluster_totss, col='blue', ylim=c(0,14), xlab='Number of clusters')
points(1:9, gap50$uniform_totss, col='red')
points(1:9, gap50$gap, col='black')
@
\caption{(Problem 3.e) The gap statistic in blue, and the in-cluster sum of squares for data with 50 extra dimensions of noise (blue) and uniform data (red)}
\end{center}
\end{figure} 

\section*{Problem 4}

<<>>=
wine = read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv", header=TRUE, sep=';')
model = Mclust(wine)
model$modelName
@

It looks like it fits the 'VVV' model to it, which fits clusters that are ellipsoidal and varying in volume, shape, and orientation. 

\subsubsection*{b}

<<>>=
model.unconstrained = Mclust(wine, modelNames=c('VVV'))
@

\subsubsection*{c}

Lets just set the number of mixture components (i.e. clusters) for EM to be the optimal number found by Mclust
<<>>=
K = model.unconstrained$G
@

And then do EM on the data!

<<>>=
library(mnormt)
n = nrow(wine)
dim = ncol(wine)
gamma = mat.or.vec(n, K)
#initialize clusters to be random around the mean of mu
muinit = rnorm(K*dim) + 8
mu = matrix(muinit, c(K, dim))

#init sigma to be identity matrices
sigmainit = runif(K*dim*dim)
sigma = array(sigmainit, dim=c(dim, dim, K))
for(i in 1:K){
  sigma[,,i] = 10*diag(dim)
}

#init pi to be uniform
pi_hat = mat.or.vec(1, K)+ 1/K

#make helper repmat function
repmat = function(X,m,n){
  ##R equivalent of repmat (matlab)
  mx = dim(X)[1]
  nx = dim(X)[2]
  matrix(t(matrix(X,mx,nx*n)),mx*m,nx*n,byrow=T)
}

#do EM for a few iterations
for(i in 1:10){

  #E STEP - calculate gamma
  #first, calculate likelihood of all data points for all clusters
  phi = mat.or.vec(n, 0)
    for(j in 1:K){
      cur_phi = dmnorm(wine, mean=mu[j,], varcov=sigma[,,j], log=FALSE)
      phi = cbind(phi, cur_phi)
    }
  ##now, calculate denominator for gamma (sums of all likelihoods time pi hat)
  phi_sum = phi %*% t(pi_hat)
  ##calculate numerator for gamma (each points likelihood times pi_hat for that class)
  phi_numerator = phi * repmat(pi_hat, n, 1)
  ## recalculate gamma
  gamma = phi_numerator/(repmat(phi_sum, 1, K))
  ## make sure nan values are set back to uniform prob
  gamma[is.nan(gamma)] <- 1/K


  #### M STEP - recalculate mu, sigma, and pi_hat
  ## get sum of gamma for each cluster
  gamma_sums = colSums(gamma, na.rm = TRUE)
  ## get each point scaled by each cluster, recalc means
  for(k in 1:K){
      clust_k_contribs = as.matrix(gamma[,k])
      clust_k_scaled_data = wine * repmat(clust_k_contribs, 1, dim)
      mu[k,] = colSums(clust_k_scaled_data, na.rm=TRUE) / gamma_sums[k]
  }

  ##recalc sigma
  for(k in 1:K){
  ### sum up numerators
    clust_k_contribs = as.matrix(gamma[,k])
    sumK = mat.or.vec(dim, dim)
    for(index in 1:n){
      mean_diff = t(as.matrix(wine[index,] - mu[k,]))
      sumK = sumK + clust_k_contribs[index] * (mean_diff %*% t(mean_diff))
    }
    sigma[,,k] = sumK/gamma_sums[k]
  }

  ## recalc pi_hat
  pi_hat = t(as.matrix(gamma_sums/n)) 
}
@

\subsubsection*{d}

It seems like they actually converge on similar solutions! Just on a quick scan comparing the highest weighted clusters of my model and the 'VVV' model, it looks like they are somewhat similar. Awesome!

<<>>=
  pi_hat
  t(mu)
  model.unconstrained$parameters$pro
  model.unconstrained$parameters$mean
@


\end{document}
